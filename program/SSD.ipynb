{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pickle\n",
    "from ssd.augmentations import SSDAugmentation\n",
    "from ssd.voc0712 import VOCDetection,VOCAnnotationTransform\n",
    "from ssd.ssd import build_ssd\n",
    "from ssd.multibox_loss import MultiBoxLoss\n",
    "from ssd.eval import evaluate_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder=os.path.expanduser('~/model/ssd/')\n",
    "cfg={\n",
    "    'num_classes': 21,\n",
    "    'lr_steps': (80000, 100000, 120000),\n",
    "    'max_iter': 120000,\n",
    "    'feature_maps': [38, 19, 10, 5, 3, 1],\n",
    "    'min_dim': 300,\n",
    "    'steps': [8, 16, 32, 64, 100, 300],\n",
    "    'min_sizes': [30, 60, 111, 162, 213, 264],\n",
    "    'max_sizes': [60, 111, 162, 213, 264, 315],\n",
    "    'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
    "    'variance': [0.1, 0.2],\n",
    "    'clip': True,\n",
    "    'name': 'VOC',\n",
    "}\n",
    "labelmap=(  # always index 0\n",
    "    'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "    'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "    'cow', 'diningtable', 'dog', 'horse',\n",
    "    'motorbike', 'person', 'pottedplant',\n",
    "    'sheep', 'sofa', 'train', 'tvmonitor')\n",
    "MEANS = (104, 117, 123)\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        init.xavier_uniform_(m.weight.data)\n",
    "        m.bias.data.zero_()\n",
    "def detection_collate(batch):\n",
    "    targets = []\n",
    "    imgs = []\n",
    "    for sample in batch:\n",
    "        imgs.append(sample[0])\n",
    "        targets.append(torch.FloatTensor(sample[1]))\n",
    "    return torch.stack(imgs, 0), targets\n",
    "\n",
    "def base_transform(image, size, mean):\n",
    "    x = cv2.resize(image, (size, size)).astype(np.float32)\n",
    "    x -= mean\n",
    "    x = x.astype(np.float32)\n",
    "    return x\n",
    "\n",
    "class BaseTransform:\n",
    "    def __init__(self, size, mean):\n",
    "        self.size = size\n",
    "        self.mean = np.array(mean, dtype=np.float32)\n",
    "\n",
    "    def __call__(self, image, boxes=None, labels=None):\n",
    "        return base_transform(image, self.size, self.mean), boxes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    dataset_root=os.path.expanduser('~/data/VOCdevkit/')\n",
    "    dataset=VOCDetection(root=dataset_root,transform=SSDAugmentation(cfg['min_dim'],MEANS))\n",
    "    net=build_ssd('train',cfg['min_dim'],cfg['num_classes'])\n",
    "    net.vgg.load_state_dict(torch.load(save_folder+'vgg16_reducedfc.pth'))\n",
    "    net.extras.apply(weights_init)\n",
    "    net.loc.apply(weights_init)\n",
    "    net.conf.apply(weights_init)\n",
    "    device=torch.device('cuda:0')\n",
    "    net=net.to(device)\n",
    "    optimizer=optim.SGD(net.parameters(),lr=1e-3,weight_decay=5e-4,momentum=0.9)\n",
    "    criterion=MultiBoxLoss(cfg['num_classes'],0.5,True,0,True,3,0.5,False,True)\n",
    "    net.train()\n",
    "    data_iter=iter(data.DataLoader(dataset,batch_size=32,num_workers=4,shuffle=True,pin_memory=True,collate_fn=detection_collate))\n",
    "    for iteration in range(cfg['max_iter']):\n",
    "        images,targets=next(data_iter)\n",
    "        images,targets=images.to(device),[ann.to(device) for ann in targets]\n",
    "        out=net(images)\n",
    "        optimizer.zero_grad()\n",
    "        loss_l,loss_c=criterion(out,targets)\n",
    "        loss=loss_l+loss_c\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    torch.save(net.state_dict(),save_folder+'ssd_voc.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval():\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    dataset_root=os.path.expanduser('~/data/VOCdevkit/')\n",
    "    num_classes=len(labelmap)+1\n",
    "    net=build_ssd('test',300,num_classes)\n",
    "    net.load_state_dict(torch.load(save_folder+'ssd300_mAP_77.43_v2.pth'))\n",
    "    net.eval()\n",
    "    dataset=VOCDetection(dataset_root,[('2007','test')],BaseTransform(300,MEANS),\n",
    "                         VOCAnnotationTransform())\n",
    "    device=torch.device('cuda:0')\n",
    "    net=net.to(device)\n",
    "    num_images=len(dataset)\n",
    "    all_boxes = [[[] for _ in range(num_images)]\n",
    "                 for _ in range(len(labelmap)+1)]\n",
    "    output_dir = '../ssd_eval'\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    det_file=os.path.join(output_dir,'detections.pkl')\n",
    "    for i in range(1):\n",
    "        im,gt,h,w=dataset.pull_item(i)\n",
    "        x=im.unsqueeze(0)\n",
    "        x=x.to(device)\n",
    "        detections=net(x).data\n",
    "        for j in  range(1,detections.size(1)):\n",
    "            dets=detections[0,j,:]\n",
    "            mask = dets[:, 0].gt(0.).expand(5, dets.size(0)).t()\n",
    "            dets = torch.masked_select(dets, mask).view(-1, 5)\n",
    "            if dets.dim()==0:\n",
    "                continue\n",
    "            boxes=dets[:,1:]\n",
    "            boxes[:, 0] *= w\n",
    "            boxes[:, 2] *= w\n",
    "            boxes[:, 1] *= h\n",
    "            boxes[:, 3] *= h\n",
    "            scores = dets[:, 0].cpu().numpy()\n",
    "            cls_dets = np.hstack((boxes.cpu().numpy(),\n",
    "                                  scores[:, np.newaxis])).astype(np.float32,\n",
    "                                                                 copy=False)\n",
    "            all_boxes[j][i] = cls_dets\n",
    "    with open(det_file,'wb') as f:\n",
    "        pickle.dump(all_boxes,f,pickle.HIGHEST_PROTOCOL)\n",
    "    evaluate_detections(all_boxes, output_dir, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing aeroplane VOC results file\n",
      "Writing bicycle VOC results file\n",
      "Writing bird VOC results file\n",
      "Writing boat VOC results file\n",
      "Writing bottle VOC results file\n",
      "Writing bus VOC results file\n",
      "Writing car VOC results file\n",
      "Writing cat VOC results file\n",
      "Writing chair VOC results file\n",
      "Writing cow VOC results file\n",
      "Writing diningtable VOC results file\n",
      "Writing dog VOC results file\n",
      "Writing horse VOC results file\n",
      "Writing motorbike VOC results file\n",
      "Writing person VOC results file\n",
      "Writing pottedplant VOC results file\n",
      "Writing sheep VOC results file\n",
      "Writing sofa VOC results file\n",
      "Writing train VOC results file\n",
      "Writing tvmonitor VOC results file\n",
      "VOC07 metric? Yes\n"
     ]
    },
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-389a9a8c7e3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-cc10b7626a1a>\u001b[0m in \u001b[0;36meval\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdet_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_boxes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mevaluate_detections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/动手学深度学习/program/ssd/eval.py\u001b[0m in \u001b[0;36mevaluate_detections\u001b[0;34m(box_list, output_dir, dataset)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_detections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0mwrite_voc_results_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m     \u001b[0mdo_python_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/动手学深度学习/program/ssd/eval.py\u001b[0m in \u001b[0;36mdo_python_eval\u001b[0;34m(output_dir, use_07)\u001b[0m\n\u001b[1;32m    191\u001b[0m         rec, prec, ap = voc_eval(\n\u001b[1;32m    192\u001b[0m            \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannopath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgsetpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcachedir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m            ovthresh=0.5, use_07_metric=use_07_metric)\n\u001b[0m\u001b[1;32m    194\u001b[0m         \u001b[0maps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0map\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'AP for {} = {:.4f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/动手学深度学习/program/ssd/eval.py\u001b[0m in \u001b[0;36mvoc_eval\u001b[0;34m(detpath, annopath, imagesetfile, classname, cachedir, ovthresh, use_07_metric)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcachefile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mrecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;31m# extract gt objects for this class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
