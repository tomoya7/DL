{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mxnet import nd ,autograd\n",
    "from mxnet.gluon import nn ,data as gdata,loss as gloss\n",
    "from mxnet import gluon\n",
    "import gluonbook as gb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv('../data/train.csv')\n",
    "test_data=pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features=pd.concat([train_data.iloc[:,1:-1],test_data.iloc[:,1:]],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberic_features=all_features.dtypes[all_features.dtypes!='object'].index\n",
    "all_features[numberic_features]=all_features[numberic_features].apply(lambda x :(x-x.mean())/x.std())\n",
    "all_features=all_features.fillna(all_features[numberic_features].mean())\n",
    "all_features=pd.get_dummies(all_features,dummy_na=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train=train_data.shape[0]\n",
    "train_labels=nd.array(train_data['SalePrice'].values)\n",
    "train_features=nd.array(all_features[:n_train].values)\n",
    "test_features=nd.array(all_features[n_train:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tain_valid_data(k,i,X,y):\n",
    "    fold_size=y.shape[0]//k\n",
    "    train_features,train_labels=None,None\n",
    "    for j in range(k):\n",
    "        index=slice(j*fold_size,(j+1)*fold_size)\n",
    "        feature_part,label_part=X[index,:],y[index]\n",
    "        if j==i:\n",
    "            valid_features,valid_labels=feature_part,label_part\n",
    "        elif train_labels is None:\n",
    "            train_features,train_labels=feature_part,label_part\n",
    "        else:\n",
    "            train_features=nd.concat(train_features,feature_part,dim=0)\n",
    "            train_labels=nd.concat(train_labels,label_part,dim=0)\n",
    "    return train_features,train_labels,valid_features,valid_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_net():\n",
    "    net=nn.Sequential()\n",
    "    net.add(nn.Dense(1))\n",
    "    net.initialize()\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train & eval (cross validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_rmse(y_pred,y):\n",
    "    rmse=nd.sqrt(loss(y_pred.log(),y.log()).mean())\n",
    "    return rmse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net,train_features,train_labels,test_features,test_labels,num_epochs,batch_size,lr,weight_dacay):\n",
    "    train_ls,test_ls=[],[]\n",
    "    train_iter=gdata.DataLoader(gdata.ArrayDataset(train_features,train_labels),batch_size,shuffle=True)\n",
    "    trainer=gluon.Trainer(net.collect_params(),'adam',{'learning_rate':lr,'wd':weight_dacay})\n",
    "    for epoch in range(num_epochs):\n",
    "        for X,y in train_iter:\n",
    "            with autograd.record():\n",
    "                l=loss(net(X),y)\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "        train_ls.append(log_rmse(net(train_features),train_labels).asscalar())\n",
    "        if test_labels is not None:\n",
    "            test_ls.append(log_rmse(net(test_features),test_labels).asscalar())\n",
    "    return train_ls,test_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(k,train_features,train_labels,num_epochs,batch_size,lr,weight_dacay):\n",
    "    train_ls_sum,valid_ls_sum=0,0\n",
    "    for i in range(k):\n",
    "        data=get_tain_valid_data(k,i,train_features,train_labels)\n",
    "        net=get_net()\n",
    "        train_ls,valid_ls=train(net,*data,num_epochs,batch_size,lr,weight_dacay)\n",
    "        train_ls_sum+=train_ls[-1]\n",
    "        valid_ls_sum+=valid_ls[-1]\n",
    "        print('fold %d train rmse: %f valid rmse :%f'%(i,train_ls[-1],valid_ls[-1]))\n",
    "    return train_ls_sum,valid_ls_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0 train rmse: 0.120007 valid rmse :0.110888\n",
      "fold 1 train rmse: 0.114759 valid rmse :0.133980\n",
      "fold 2 train rmse: 0.115814 valid rmse :0.118801\n",
      "fold 3 train rmse: 0.118628 valid rmse :0.109448\n",
      "fold 4 train rmse: 0.115191 valid rmse :0.129350\n",
      "avg train rmse: 0.116880 avg valid rmse :0.120493\n"
     ]
    }
   ],
   "source": [
    "k,num_epochs,batch_size,lr,weight_decay=5,100,64,5,0\n",
    "loss=gloss.L2Loss()\n",
    "train_ls_sum,valid_ls_sum=cross_validation(k,train_features,train_labels,num_epochs,batch_size,lr,weight_decay)\n",
    "train_l,valid_l=train_ls_sum/k,valid_ls_sum/k\n",
    "print('avg train rmse: %f avg valid rmse :%f'%(train_l,valid_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(train_features,train_labels,test_features,num_epochs,batch_size,lr,weight_decay):\n",
    "    net=get_net()\n",
    "    train_ls,_=train(net,train_features,train_labels,None,None,num_epochs,batch_size,lr,weight_decay)\n",
    "    print('train rmse is',t                                                              1])\n",
    "    preds=net(test_features).asnumpy()\n",
    "    print(preds.shape)\n",
    "    test_data['SalePrice']=pd.Series(preds[:,0])\n",
    "    submission=pd.concat([test_data['Id'],test_data['SalePrice']],axis=1)\n",
    "    submission.to_csv('submission.csv',index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse is 0.11516944\n",
      "(1459, 1)\n"
     ]
    }
   ],
   "source": [
    "prediction(train_features,train_labels,test_features,num_epochs,batch_size,lr,weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
